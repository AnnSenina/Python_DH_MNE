{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW2hpeXBalaeqhUHFITN2S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_DH_MNE/blob/main/notebooks/Python_9_%D0%B1%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B8_%D0%B4%D0%BB%D1%8F_DH_%D1%82%D0%B5%D0%BA%D1%81%D1%82.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Анализ текстов"
      ],
      "metadata": {
        "id": "j_3P4hI5OB1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# все установки библиотек\n",
        "!pip install wordcloud\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install eng-spacysentiment\n",
        "!pip install textblob\n",
        "!pip install dostoevsky\n",
        "!python -m dostoevsky download fasttext-social-network-model"
      ],
      "metadata": {
        "id": "ikhj9xmJOiJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2J7gR9nLg4t"
      },
      "outputs": [],
      "source": [
        "# для очистки (препроцессинга) текста...\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "ru_stop_words = stopwords.words('russian') # давайте для русского сохранять в отдельную переменную\n",
        "\n",
        "# стемминг\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "ru_stemmer = SnowballStemmer(\"russian\") # с русским работает плохл\n",
        "\n",
        "# spaCy лемматизация английского\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 1 модель сентимент-анализа из коробки от spacy\n",
        "import spacy\n",
        "import eng_spacysentiment\n",
        "# альтернатива - сентимент-анализ с TextBlob\n",
        "from textblob import TextBlob\n",
        "# сентимент-анализ текстов на русском\n",
        "from dostoevsky.tokenization import RegexTokenizer\n",
        "from dostoevsky.models import FastTextSocialNetworkModel\n",
        "tokenizer = RegexTokenizer()\n",
        "model = FastTextSocialNetworkModel(tokenizer=tokenizer)\n",
        "\n",
        "#таблички\n",
        "import pandas as pd\n",
        "\n",
        "# для рисования облака слов\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# для подсчета любых частот (часто встречающиеся слова, n-граммы и т.д.)\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# русский язык не лемматизируется NLTK - берем другие библиотеки\n",
        "\n",
        "# pymorphy\n",
        "!pip install pymorphy3 # недавно попробовала 3 версию, но и во 2 все равботает аналогично\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "# mystem от Яндекса\n",
        "!pip install pymystem3\n",
        "from pymystem3 import Mystem\n",
        "mystem = Mystem()"
      ],
      "metadata": {
        "id": "MgSAJb02QijP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Документация:\n",
        "\n",
        "* [NLTK](https://www.nltk.org/index.html)\n",
        "* [pymorphy](https://pymorphy2.readthedocs.io/en/stable/), таблица граммем\n",
        "* [mystem](https://yandex.ru/dev/mystem/)\n",
        "* [wordcloud](https://github.com/amueller/word_cloud)\n",
        "* [spacy](https://github.com/explosion/spaCy), [сентимент-анализ](https://spacy.io/universe/project/eng_spacysentiment) в spacy\n",
        "* [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
        "* [dostoevsky](https://github.com/bureaucratic-labs/dostoevsky)"
      ],
      "metadata": {
        "id": "o4egzYQJPZLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Начнем с английского"
      ],
      "metadata": {
        "id": "neVr8ApzT6Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('01 - The Fellowship Of The Ring.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "text"
      ],
      "metadata": {
        "id": "wdHCxS6OUAqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.replace('\\ufeff', '')\n",
        "text = text.lower() # первый шаг - перевод к нижнему регистру\n",
        "text"
      ],
      "metadata": {
        "id": "KxIqPsAxsJcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# в зависимости от задач можно разделить текст сразу на токены\n",
        "# или на предложения, а затем на токены"
      ],
      "metadata": {
        "id": "KNZl5_FdsVX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens = word_tokenize(text) #NLTK\n",
        "text_tokens # список слов"
      ],
      "metadata": {
        "id": "SDfUY5mnsay2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_sent = sent_tokenize(text)\n",
        "print(text_sent) # список предложений\n",
        "text_sent_tokens = []\n",
        "for i in text_sent:\n",
        "  l = word_tokenize(i)\n",
        "  text_sent_tokens.append(l)\n",
        "text_sent_tokens # список списков: предложения и слова"
      ],
      "metadata": {
        "id": "8Y1R6uWfsaxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# в токенах осталась пункуация с топ слова\n",
        "stop_words # список стоп-слов из NLTK\n",
        "# при необходимости его можно расширять так:\n",
        "# stop_words = stop_words + ['ваше_слово1', 'ваше_слово2'...]"
      ],
      "metadata": {
        "id": "7RpyPGCVs5ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tokens = [] # чистим список токенов\n",
        "for i in text_tokens:\n",
        "  if i[0].isalpha() and i not in stop_words:\n",
        "    clean_tokens.append(i)\n",
        "clean_tokens"
      ],
      "metadata": {
        "id": "QtUgZ-5Rukzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Давайте два разных способа очистки объединим в функции\n",
        "def get_clean_tokens(text):\n",
        "  text = text.lower()\n",
        "  text_tokens = word_tokenize(text)\n",
        "  clean_tokens = [] # чистим список токенов\n",
        "  for i in text_tokens:\n",
        "    if i[0].isalpha() and i not in stop_words:\n",
        "      clean_tokens.append(i)\n",
        "  return clean_tokens\n",
        "\n",
        "example = get_clean_tokens(text)\n",
        "example\n",
        "# теперь вы можете отправлять туда текст и получать список токенов"
      ],
      "metadata": {
        "id": "v5jW1WXayoK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# не будем писать отдельную функцию - просто разделим на предложения и получим токены\n",
        "example_2 = []\n",
        "for i in sent_tokenize(text):\n",
        "  s = get_clean_tokens(i)\n",
        "  if len(s) > 0:\n",
        "    example_2.append(s)\n",
        "example_2\n",
        "\n",
        "# зачем нам предложения? Пока оставим их, они пригодятся)"
      ],
      "metadata": {
        "id": "TqQ3mnPd138v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для русского можно использовать такую же функцию, но передать стоп-слова на русском!"
      ],
      "metadata": {
        "id": "NIlCOmAa5RJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Токены чистые - давайте стеммить и лемматизировать"
      ],
      "metadata": {
        "id": "rs57cCV32mRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stem_tokens(text_list):\n",
        "  text_stemmed = []\n",
        "  for i in text_list:\n",
        "    text_stemmed.append(stemmer.stem(i))\n",
        "  return text_stemmed\n",
        "\n",
        "get_stem_tokens(example) # для русского аналогично - но результат будет хуже"
      ],
      "metadata": {
        "id": "10eRuAZP2rlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лемматизация - гораздо важнее и интереснее"
      ],
      "metadata": {
        "id": "xK_UOzFp5_mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemmas(text_list):\n",
        "  spacy_token = []\n",
        "  doc = nlp(' '.join(text_list))\n",
        "  for token in doc:\n",
        "    if token.lemma_ != '-' and token.lemma_ != '.':\n",
        "        spacy_token.append(token.lemma_)\n",
        "  return spacy_token\n",
        "\n",
        "example_lem = get_lemmas(example)\n",
        "example_lem"
      ],
      "metadata": {
        "id": "hb6T9b1Z6DSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_lem_2 = []\n",
        "for i in example_2:\n",
        "  s = get_lemmas(i)\n",
        "  if len(s) > 0:\n",
        "    example_lem_2.append(s)\n",
        "example_lem_2\n",
        "# давайте для одного предложения проведем сентимент-анализ"
      ],
      "metadata": {
        "id": "oEP-07NO7DZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(example_lem_2[0], columns=['token'])\n",
        "df"
      ],
      "metadata": {
        "id": "4MeEzGMl7Unj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy\n",
        "nlp = eng_spacysentiment.load()\n",
        "\n",
        "spacy_pos = []\n",
        "spacy_neg = []\n",
        "for i in df.token:\n",
        "  doc = nlp(i)\n",
        "  spacy_pos.append(round(doc.cats['positive'], 5))\n",
        "  spacy_neg.append(round(doc.cats['negative'], 5))\n",
        "\n",
        "df['spacy_pos'] = spacy_pos\n",
        "df['spacy_neg'] = spacy_neg\n",
        "df"
      ],
      "metadata": {
        "id": "ciVCwlSX7lSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# теперь попробуем другой бибилиотекой!\n",
        "# у нее другие параметры: оценка настроения + оценка на субъективность\n",
        "\n",
        "blob_polar = []\n",
        "blob_subj = []\n",
        "for i in df.token:\n",
        "  analysis = TextBlob(i).sentiment\n",
        "  blob_polar.append(round(analysis[0], 5))\n",
        "  blob_subj.append(round(analysis[1], 5))\n",
        "\n",
        "df['blob_polar'] = blob_polar\n",
        "df['blob_subj'] = blob_subj\n",
        "df"
      ],
      "metadata": {
        "id": "nVOTe1ly7r4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обычно так поступают с предложениями, а не отдельными словами, если текст большой"
      ],
      "metadata": {
        "id": "A8gXUPoW72y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = []\n",
        "for i in example_lem_2:\n",
        "  i = ' '.join(i)\n",
        "  sent.append(i)\n",
        "sent"
      ],
      "metadata": {
        "id": "sQR4ZA-o78jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.DataFrame(sent, columns=['token'])\n",
        "spacy_pos = []\n",
        "spacy_neg = []\n",
        "for i in df2.token:\n",
        "  doc = nlp(i)\n",
        "  spacy_pos.append(round(doc.cats['positive'], 5))\n",
        "  spacy_neg.append(round(doc.cats['negative'], 5))\n",
        "\n",
        "df2['spacy_pos'] = spacy_pos\n",
        "df2['spacy_neg'] = spacy_neg\n",
        "\n",
        "blob_polar = []\n",
        "blob_subj = []\n",
        "for i in df2.token:\n",
        "  analysis = TextBlob(i).sentiment\n",
        "  blob_polar.append(round(analysis[0], 5))\n",
        "  blob_subj.append(round(analysis[1], 5))\n",
        "\n",
        "df2['blob_polar'] = blob_polar\n",
        "df2['blob_subj'] = blob_subj\n",
        "df2"
      ],
      "metadata": {
        "id": "B5RWwYaD8e0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Частоты"
      ],
      "metadata": {
        "id": "rrNt71ie8-Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(example_lem).most_common(50)"
      ],
      "metadata": {
        "id": "BfEplDbL9AJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##N-gramms\n",
        "Текст можно разделить на n-граммы – устойчивые сочетания по N слов:\n",
        "\n",
        "    nltk.bigrams() – сочетания по два слова\n",
        "    nltk.trigrams() – сочетания по три слова\n",
        "    nltk.ngrams(list, n) – сочетания по N слов"
      ],
      "metadata": {
        "id": "dgp4eGlH9QEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_bigramms = Counter(nltk.bigrams(example_lem))\n",
        "freq_bigramms.most_common(20)"
      ],
      "metadata": {
        "id": "BaxE8Sj_9FQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_ngramms = Counter(nltk.ngrams(example_lem, 4))\n",
        "freq_nigramms.most_common(10)"
      ],
      "metadata": {
        "id": "IDQSLcia9Y0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Облако слов"
      ],
      "metadata": {
        "id": "2ww7goPb9fs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерируем облако слов\n",
        "wordcloud = WordCloud().generate(', '.join(example_lem))\n",
        "plt.imshow(wordcloud) # Что изображаем\n",
        "plt.axis(\"off\") # Без подписей на осях\n",
        "plt.show() # показать изображение"
      ],
      "metadata": {
        "id": "IgxylthE9oaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(width = 2000,\n",
        "                      height = 1500,\n",
        "                      background_color='black',\n",
        "                      colormap='Pastel1').generate(', '.join(example_lem))\n",
        "plt.figure(figsize=(30, 15)) # Устанавливаем размер картинки\n",
        "plt.imshow(wordcloud) # Что изображаем\n",
        "plt.axis(\"off\") # Без подписей на осях\n",
        "plt.show() # показать изображение"
      ],
      "metadata": {
        "id": "kN38Tw3A9sqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Попробуем с русским"
      ],
      "metadata": {
        "id": "VYYR9eHvT9NB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отличаться будет только лемматизация (NLTK не лемматизирует русский)\n"
      ],
      "metadata": {
        "id": "yyApuR0C-aO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# сначала простые примеры\n",
        "morph.parse('человек')"
      ],
      "metadata": {
        "id": "mmk04pz7-hkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# сделаем красиво\n",
        "print('Cлово - ', morph.parse('человеком')[0].word)\n",
        "print('Лемма слова - ', morph.parse('человеком')[0].normal_form)\n",
        "print('Грамматическая информация слова - ', morph.parse('человеком')[0].tag)\n",
        "print('Часть речи слова - ', morph.parse('человеком')[0].tag.POS)\n",
        "print('Род слова - ', morph.parse('человеком')[0].tag.gender)\n",
        "print('Число слова - ', morph.parse('человеком')[0].tag.number)\n",
        "print('Падеж слова - ', morph.parse('человеком')[0].tag.case)"
      ],
      "metadata": {
        "id": "79WTxQcS-tCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# для лемматизации нужна именно эта команда\n",
        "morph.parse('человеком')[0].normal_form"
      ],
      "metadata": {
        "id": "0B69le74_CTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Важно!\n",
        "pymorphy не умеет сам токенизировать\n",
        "\n",
        "Пробуем на реальном тексте"
      ],
      "metadata": {
        "id": "oBfOvOob_Hmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Dostoevsky_PrestuplenieINakazanie.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "text"
      ],
      "metadata": {
        "id": "u9ifa85I9-ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clean_tokens_ru(text):\n",
        "  text = text.lower()\n",
        "  text_tokens = word_tokenize(text)\n",
        "  clean_tokens = [] # чистим список токенов\n",
        "  for i in text_tokens:\n",
        "    if i[0].isalpha() and i not in ru_stop_words: # отличие здесь!\n",
        "      clean_tokens.append(i)\n",
        "  return clean_tokens\n",
        "\n",
        "clean_text_ru = get_clean_tokens_ru(text)\n",
        "clean_text_ru"
      ],
      "metadata": {
        "id": "zmbgsllM_RYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ru_lemmatized_1 = []\n",
        "for word in clean_text_ru:\n",
        "    result = morph.parse(word)\n",
        "    most_probable_result = result[0] ## почему мы берем первый разбор? см.в этом месте: https://pymorphy2.readthedocs.io/en/latest/user/guide.html#select-correct\n",
        "    normal_form = most_probable_result.normal_form\n",
        "    ru_lemmatized_1.append(normal_form)\n",
        "\n",
        "ru_lemmatized_1"
      ],
      "metadata": {
        "id": "Qt9ZjH8j_kVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MyStem"
      ],
      "metadata": {
        "id": "xrT0a_4T_8Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mystem лемматизирует сам\n",
        "mystem.lemmatize('mystem даже сам умеет токенизировать текст')"
      ],
      "metadata": {
        "id": "e6vq0DRS_-fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_lemmatized_mystem = mystem.lemmatize(text.lower())\n",
        "words_lemmatized_mystem"
      ],
      "metadata": {
        "id": "yTmrwaZm9-OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "for i in words_lemmatized_mystem:\n",
        "  if i[0].isalpha() and i not in ru_stop_words:\n",
        "    words.append(i)\n",
        "words"
      ],
      "metadata": {
        "id": "kSfgQW2-A0Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Остался сентимент-анализ"
      ],
      "metadata": {
        "id": "PxDJK6s8BB64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.predict(words[:30], k=2)\n",
        "\n",
        "for word, sentiment in zip(words, results):\n",
        "    print(word, '->', sentiment)"
      ],
      "metadata": {
        "id": "hJZ-3FM1BE01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# или по предложениям\n",
        "example_ru = []\n",
        "for i in sent_tokenize(text):\n",
        "  s = get_clean_tokens_ru(i)\n",
        "  if len(s) > 0:\n",
        "    example_ru.append(s)\n",
        "example_ru"
      ],
      "metadata": {
        "id": "yejL9QPcBhL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ru_lemmatized_sent = []\n",
        "for s in example_ru:\n",
        "  sent_lem = []\n",
        "  for w in s:\n",
        "    result = morph.parse(w)\n",
        "    most_probable_result = result[0]\n",
        "    normal_form = most_probable_result.normal_form\n",
        "    sent_lem.append(normal_form)\n",
        "  ru_lemmatized_sent.append(' '.join(sent_lem))\n",
        "\n",
        "ru_lemmatized_sent"
      ],
      "metadata": {
        "id": "la9knA-mChW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.predict(ru_lemmatized_sent[:20], k=2)\n",
        "\n",
        "for sentence, sentiment in zip(ru_lemmatized_sent, results):\n",
        "    print(sentence, '->', sentiment)"
      ],
      "metadata": {
        "id": "8Bby_vVCEKaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Бонус:\n",
        "\n",
        "так будет выглядеть функция, лемматизирующая английский / русский"
      ],
      "metadata": {
        "id": "PJMDHoTetV1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_en_lemm(text):\n",
        "  text = text.lower()\n",
        "  text_tokens = word_tokenize(text)\n",
        "  clean_tokens = [] # чистим список токенов\n",
        "  for i in text_tokens:\n",
        "    if i[0].isalpha() and i not in stop_words:\n",
        "      clean_tokens.append(i)\n",
        "  spacy_token = []\n",
        "  doc = nlp(' '.join(clean_tokens))\n",
        "  for token in doc:\n",
        "    if token.lemma_ != '-' and token.lemma_ != '.':\n",
        "        spacy_token.append(token.lemma_)\n",
        "  return spacy_token\n",
        "\n",
        "# передаете текст на английском строкой, получаете список лемм\n",
        "# пример:\n",
        "some_text = '''There are very many things to enjoy in every season. Each year starts with joyful winter holidays. Big, white snowflakes are falling thick and fast and soon the ground, the roofs and the trees are covered with snow. Children and grown-ups may make snowmen, play snow balls, go skiing, skating or sliding down the snow covered hills.'''\n",
        "\n",
        "print(get_en_lemm(some_text))"
      ],
      "metadata": {
        "id": "9da2OYmmtdQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ru_lemm(text):\n",
        "  text = text.lower()\n",
        "  text_tokens = word_tokenize(text)\n",
        "  clean_tokens = [] # чистим список токенов\n",
        "  for i in text_tokens:\n",
        "    if i[0].isalpha() and i not in ru_stop_words: # отличие здесь!\n",
        "      clean_tokens.append(i)\n",
        "  ru_lemmatized = []\n",
        "  for word in clean_tokens:\n",
        "      ru_lemmatized.append(morph.parse(word)[0].normal_form)\n",
        "  return ru_lemmatized\n",
        "\n",
        "some_text = \"\"\"По результатам оценки критериев вузам присвоили один из рейтинговых статусов. Высшая школа экономики вошла в лидерскую группу А+, заняв первое место по таким критериям, как «Востребованность выпускников в найме», «Качество образовательной среды» и «Активность по развитию школьного образования».\n",
        "Презентация рейтинга прошла в Москве на международной конференции AI Journey 2023. Один из основных критериев оценки вузов при расчете рейтинга — размер заработной платы молодых специалистов в течение года после завершения обучения (при трудоустройстве по специальности). В вузах-лидерах рейтинга средняя зарплата выпускников составила около 140 тыс. рублей.\n",
        "В НИУ ВШЭ с 2021 года действует Центр искусственного интеллекта. Миссия Центра развитие и внедрение технологий искусственного интеллекта в разные сферы жизни человека и общества, отрасли науки и сектора экономики. НИУ ВШЭ стал одним из победителей конкурса на получение гранта от Правительства Российской Федерации для создания центра искусственного интеллекта.\n",
        "Конкурс проводился в рамках федерального проекта «Искусственный интеллект», основная задача которого — стимулирование развития и внедрения технологий ИИ в России.\n",
        "«По поручению главы государства Альянс в сфере ИИ совместно с Минобрнауки разработали рейтинг российских вузов по качеству подготовки специалистов по ИИ. Рейтинг является важнейшим индикатором качества образования в области ИИ и наглядно отражает мнение работодателей о том, насколько образовательные программы актуальны и отвечают запросу рынка. Среди 180 вузов, оказавшихся в рейтинге, 10 имеют оценки А+, А (хорошее качество) B+, B (приемлемое качество).\n",
        "Абсолютные лидеры — это ВШЭ, МФТИ и ИТМО. Таким образом, ТОП-10 российских университетов уже могут конкурировать за звание лучших, а значит готовят высококвалифицированных специалистов и успешно развивают науку в области искусственного интеллекта», — отметил Дмитрий Чернышенко, заместитель председателя Правительства России, куратор нацпрограммы «Цифровая экономика».\n",
        "Для объективной оценки качества специалистов по ИИ эксперты выделили 13 критериев, включая такие аспекты, как уровень заработных плат выпускников, их востребованность на рынке труда, наличие научных публикаций на конференциях и в журналах, количество призеров студенческих олимпиад, средний балл по ЕГЭ, и наличие программ, прошедших профессионально-общественную аккредитацию со стороны Альянса.\"\"\"\n",
        "\n",
        "print(get_ru_lemm(some_text))"
      ],
      "metadata": {
        "id": "7XC7cao_uY05"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}